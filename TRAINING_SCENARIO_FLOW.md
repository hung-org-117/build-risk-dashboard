# Training Scenario Pipeline - MÃ´ Táº£ Chi Tiáº¿t ToÃ n Bá»™ Luá»“ng

## ğŸ“‹ Má»¥c Lá»¥c
1. [Tá»•ng Quan Kiáº¿n TrÃºc](#tá»•ng-quan-kiáº¿n-trÃºc)
2. [Phase 0: Build Source Upload](#phase-0-build-source-upload)
3. [Phase 1: Filtering & Ingestion](#phase-1-filtering--ingestion)
4. [Phase 2: Processing & Feature Extraction](#phase-2-processing--feature-extraction)
5. [Phase 3: Dataset Generation](#phase-3-dataset-generation)
6. [Entities & Data Model](#entities--data-model)
7. [API Endpoints](#api-endpoints)
8. [Frontend UI Flow](#frontend-ui-flow)
9. [Error Handling & Recovery](#error-handling--recovery)
10. [WebSocket Real-time Updates](#websocket-real-time-updates)

---

## Tá»•ng Quan Kiáº¿n TrÃºc

### High-Level Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      TRAINING SCENARIO PIPELINE                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 0: Admin uploads CSV (Build Source)
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BUILD SOURCE VALIDATION                  â”‚
â”‚  âœ“ Parse CSV with build IDs               â”‚
â”‚  âœ“ Validate repos on GitHub API           â”‚
â”‚  âœ“ Validate builds on CI API              â”‚
â”‚  âœ“ Cache to RawRepository & RawBuildRun   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
[Build Warehouse: raw_build_runs collection]
       â”‚
       â–¼
User creates Training Scenario with filters
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 1: FILTERING & INGESTION          â”‚
â”‚  âœ“ Query builds from warehouse (filters) â”‚
â”‚  âœ“ Create TrainingIngestionBuild records â”‚
â”‚  âœ“ Clone/update git repositories         â”‚
â”‚  âœ“ Create git worktrees cho commits      â”‚
â”‚  âœ“ Download build logs tá»« CI             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼ (User triggers manually)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 2: PROCESSING                     â”‚
â”‚  âœ“ Dispatch scans (Trivy, SonarQube)     â”‚
â”‚  âœ“ Create TrainingEnrichmentBuild        â”‚
â”‚  âœ“ Extract features (Hamilton DAG)       â”‚
â”‚  âœ“ Sequential processing (temporal deps) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼ (User triggers manually)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 3: DATASET GENERATION             â”‚
â”‚  âœ“ Collect features from EnrichmentBuildsâ”‚
â”‚  âœ“ Apply splitting strategy              â”‚
â”‚  âœ“ Generate train/val/test files         â”‚
â”‚  âœ“ Export to parquet/csv/pickle          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
TrainingScenario (COMPLETED) + Dataset Splits Ready
```

### Queue Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Celery Queue System             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ validation     â”‚ Build source validationâ”‚
â”‚ ingestion      â”‚ Clone, worktree, logs  â”‚
â”‚ processing     â”‚ Feature extraction     â”‚
â”‚ trivy_scan     â”‚ Trivy security scans   â”‚
â”‚ sonar_scan     â”‚ SonarQube analysis     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Status Flow

```
TrainingScenario Status Flow:

    QUEUED â”€â”€â–º FILTERING â”€â”€â–º INGESTING â”€â”€â–º INGESTED
                  â”‚              â”‚             â”‚
                  â–¼              â–¼             â”‚
               FAILED â—„â”€â”€â”€â”€â”€â”€ FAILED          â”‚
                  â”‚                            â–¼
                  â–¼                 (User triggers processing)
             (Retry available)                 â”‚
                                               â–¼
                                    â”Œâ”€â”€â”€ PROCESSING â”€â”€â”€â”
                                    â”‚                  â”‚
                                    â–¼                  â–¼
                                PROCESSED           FAILED
                                    â”‚
                                    â–¼
                         (User triggers split/export)
                                    â”‚
                                    â–¼
                         â”Œâ”€â”€â”€ SPLITTING â”€â”€â”€â”
                         â”‚                 â”‚
                         â–¼                 â–¼
                     COMPLETED          FAILED
```

---

## Phase 0: Build Source Upload

**Files**: 
- [backend/app/api/build_sources.py](backend/app/api/build_sources.py)
- [backend/app/tasks/source_validation.py](backend/app/tasks/source_validation.py)

**Má»¥c Ä‘Ã­ch**: Thu tháº­p validated builds vÃ o warehouse (raw_build_runs)

### 0.1 Tasks Overview

| Task | Queue | Timeout | MÃ´ Táº£ |
|------|-------|---------|-------|
| `source_validation_orchestrator` | validation | 3600s | Parse CSV, dispatch validation |
| `validate_source_repos` | validation | 600s | Validate repos on GitHub |
| `validate_source_builds` | validation | 600s | Validate builds on CI |

### 0.2 Upload Flow

```
Admin uploads CSV (build_id, repo_name columns)
       â”‚
       â–¼
POST /api/build-sources/upload
â”‚
â”œâ”€ Parse CSV, create BuildSource entity
â”œâ”€ Create SourceBuild records (PENDING)
â””â”€ Dispatch source_validation_orchestrator
       â”‚
       â–¼
source_validation_orchestrator
â”‚
â”œâ”€ Group builds by repo
â”œâ”€ For each repo:
â”‚   â”œâ”€ validate_source_repos â†’ RawRepository
â”‚   â””â”€ validate_source_builds â†’ RawBuildRun
â”‚
â””â”€ Aggregate results
    â”œâ”€ Mark SourceBuild as FOUND/NOT_FOUND
    â””â”€ Update BuildSource validation stats
```

### 0.3 Data Structures (Phase 0)

**BuildSource**:
```python
{
    name: str,
    description: Optional[str],
    file_name: str,
    rows: int,
    mapped_fields: Dict[str, str],  # CSV column mapping
    validation_status: str,         # pending, in_progress, completed
    validation_stats: {
        total: int,
        found: int,
        not_found: int,
        filtered: int,
    }
}
```

**SourceBuild**:
```python
{
    source_id: ObjectId,
    build_id_from_source: str,
    repo_name_from_source: str,
    status: "pending" | "found" | "not_found" | "filtered",
    raw_repo_id: Optional[ObjectId],   # Link to RawRepository
    raw_run_id: Optional[ObjectId],    # Link to RawBuildRun
}
```

---

## Phase 1: Filtering & Ingestion

**File**: [backend/app/tasks/training_ingestion.py](backend/app/tasks/training_ingestion.py)

**Má»¥c Ä‘Ã­ch**: Lá»c builds tá»« warehouse theo config vÃ  chuáº©n bá»‹ resources

### 1.1 Tasks Overview

| Task | Queue | Timeout | MÃ´ Táº£ |
|------|-------|---------|-------|
| `start_scenario_ingestion` | processing | 180s | Orchestrator: Filter + dispatch ingestion |
| `aggregate_scenario_ingestion` | ingestion | 120s | Aggregate results, mark INGESTED |
| `handle_scenario_chord_error` | ingestion | 120s | Error callback |
| `reingest_failed_builds` | processing | 900s | Retry FAILED builds |

### 1.2 Filtering Flow

```
User creates Scenario via /scenarios/create wizard
       â”‚
       â–¼
POST /api/training-scenarios
â”‚
â”œâ”€ Parse config (data_source, features, splitting, preprocessing)
â”œâ”€ Create TrainingScenario entity (QUEUED)
â””â”€ User clicks "Start Ingestion" â†’ start_scenario_ingestion
       â”‚
       â–¼
start_scenario_ingestion
â”‚
â”œâ”€ Update status â†’ FILTERING
â”œâ”€ Query RawBuildRun by filters:
â”‚   â”œâ”€ date_start, date_end
â”‚   â”œâ”€ languages (from RawRepository.main_lang)
â”‚   â”œâ”€ conclusions (success, failure)
â”‚   â”œâ”€ ci_provider
â”‚   â””â”€ exclude_bots
â”‚
â”œâ”€ Create TrainingIngestionBuild for each matched build
â”œâ”€ Update status â†’ INGESTING
â”‚
â””â”€ chord(
       group(clone_repo â†’ worktrees â†’ logs) Ã— N repos,
       aggregate_scenario_ingestion
   )
```

### 1.3 Data Source Config

```python
DataSourceConfig = {
    filter_by: "all" | "by_language" | "by_name",
    languages: ["python", "java"],      # Filter by main_lang
    date_start: "2024-01-01",
    date_end: "2024-12-31",
    conclusions: ["success", "failure"],
    ci_provider: "all" | "github_actions" | "circleci",
    exclude_bots: True,
}
```

### 1.4 TrainingIngestionBuild Status

```
    PENDING â”€â”€â”€â–º INGESTING â”€â”€â”€â–º INGESTED
                     â”‚              â”‚
                     â–¼              â–¼
                  FAILED     (Ready for Processing)
                     â”‚
                     â–¼
              MISSING_RESOURCE
              (Not retryable)
```

---

## Phase 2: Processing & Feature Extraction

**File**: [backend/app/tasks/training_processing.py](backend/app/tasks/training_processing.py)

**Má»¥c Ä‘Ã­ch**: Extract features tá»« ingested builds

### 2.1 Tasks Overview

| Task | Queue | Timeout | MÃ´ Táº£ |
|------|-------|---------|-------|
| `start_scenario_processing` | processing | 120s | User triggers Phase 2 |
| `dispatch_scans_and_processing` | processing | 180s | Dispatch scans + feature extraction |
| `dispatch_scenario_scans` | processing | 600s | Fire-and-forget scan dispatch |
| `dispatch_enrichment_batches` | processing | 360s | Create EnrichmentBuild, dispatch chain |
| `process_single_enrichment` | processing | 900s | Extract features for 1 build |
| `finalize_scenario_processing` | processing | 120s | Mark PROCESSED |
| `reprocess_failed_builds` | processing | 360s | Retry failed builds |

### 2.2 Processing Flow

```
User clicks "Start Processing"
       â”‚
       â–¼
start_scenario_processing
â”‚
â”œâ”€ Validate status is INGESTED
â”œâ”€ Update status â†’ PROCESSING
â””â”€ dispatch_scans_and_processing
       â”‚
       â”œâ”€ dispatch_scenario_scans (async, fire & forget)
       â”‚   â””â”€ Dispatch Trivy + SonarQube for unique commits
       â”‚
       â””â”€ dispatch_enrichment_batches
           â”‚
           â”œâ”€ Create TrainingEnrichmentBuild for each INGESTED build
           â””â”€ chain(
                  process_single_enrichment(build_1),
                  process_single_enrichment(build_2),
                  ...,
                  finalize_scenario_processing
              )
```

### 2.3 Sequential Processing (Temporal Features)

Processing MUST be sequential (oldest â†’ newest) for temporal features:

```python
# Temporal features depend on previous builds
# tr_prev_build_duration, tr_success_rate_last_5, etc.

chain(
    process_single_enrichment(build_1),  # 2024-01-01
    process_single_enrichment(build_2),  # 2024-01-02 (references build_1)
    process_single_enrichment(build_3),  # 2024-01-03 (references build_2)
    ...,
    finalize_scenario_processing
)
```

### 2.4 Feature Config

```python
FeatureConfig = {
    dag_features: ["git_*", "build_*", "log_*"],  # Wildcard support
    scan_metrics: {
        sonarqube: ["code_smells", "bugs", "coverage"],
        trivy: ["vuln_total", "vuln_critical"],
    },
    extractor_configs: {
        "lookback_days": 90,
        "test_frameworks": ["pytest", "junit"],
    },
}
```

---

## Phase 3: Dataset Generation

**File**: [backend/app/tasks/training_processing.py](backend/app/tasks/training_processing.py) (`generate_scenario_dataset`)

**Má»¥c Ä‘Ã­ch**: Split vÃ  export dataset thÃ nh train/val/test files

### 3.1 Tasks Overview

| Task | Queue | Timeout | MÃ´ Táº£ |
|------|-------|---------|-------|
| `generate_scenario_dataset` | processing | 900s | Collect features, split, export |

### 3.2 Generation Flow

```
User clicks "Generate Dataset"
       â”‚
       â–¼
generate_scenario_dataset
â”‚
â”œâ”€ Validate status is PROCESSED
â”œâ”€ Update status â†’ SPLITTING
â”‚
â”œâ”€ Collect features from EnrichmentBuilds
â”‚   â”œâ”€ Query all builds with extraction_status = COMPLETED
â”‚   â”œâ”€ Join with FeatureVector.features
â”‚   â””â”€ Join with FeatureVector.scan_metrics (if available)
â”‚
â”œâ”€ Build pandas DataFrame
â”‚   â”œâ”€ Feature columns
â”‚   â”œâ”€ Label column (outcome: success/failure)
â”‚   â””â”€ Metadata (repo, commit, build_id)
â”‚
â”œâ”€ Apply preprocessing:
â”‚   â”œâ”€ Handle missing values (drop_row | fill | skip_feature)
â”‚   â””â”€ Normalize (z_score | min_max | robust | none)
â”‚
â”œâ”€ Apply splitting strategy:
â”‚   â”œâ”€ stratified_within_group (default)
â”‚   â”œâ”€ leave_one_out
â”‚   â”œâ”€ time_series_split
â”‚   â””â”€ random_split
â”‚
â”œâ”€ Export files:
â”‚   â”œâ”€ train.parquet (70%)
â”‚   â”œâ”€ val.parquet (15%)
â”‚   â””â”€ test.parquet (15%)
â”‚
â”œâ”€ Create TrainingDatasetSplit records
â”‚
â””â”€ Update status â†’ COMPLETED
```

### 3.3 Splitting Config

```python
SplittingConfig = {
    strategy: "stratified_within_group",
    group_by: "repo_name" | "language" | "ci_provider",
    stratify_by: "outcome" | "conclusion",
    ratios: {
        train: 0.7,
        val: 0.15,
        test: 0.15,
    },
    temporal_ordering: True,  # Sort by build_started_at
}
```

### 3.4 TrainingDatasetSplit

```python
{
    scenario_id: ObjectId,
    split_type: "train" | "val" | "test",
    file_path: str,
    file_format: "parquet" | "csv" | "pickle",
    file_size_bytes: int,
    record_count: int,
    feature_count: int,
    class_distribution: {"success": 500, "failure": 200},
    generated_at: datetime,
}
```

---

## Entities & Data Model

### Entity Relationship Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   BuildSource       â”‚     â”‚    SourceBuild      â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ _id                 â”‚â—„â”€â”€â”€â”€â”¤ source_id           â”‚
â”‚ name                â”‚     â”‚ build_id_from_sourceâ”‚
â”‚ validation_status   â”‚     â”‚ raw_run_id          â”‚â”€â”€â”€â”€â”
â”‚ validation_stats    â”‚     â”‚ status              â”‚    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
                                                        â”‚
                         [Warehouse]                    â”‚
                              â”‚                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   RawRepository     â”‚     â”‚    RawBuildRun      â”‚â—„â”€â”€â”€â”˜
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ _id                 â”‚â—„â”€â”€â”€â”€â”¤ raw_repo_id         â”‚
â”‚ full_name           â”‚     â”‚ _id                 â”‚
â”‚ github_repo_id      â”‚     â”‚ ci_run_id           â”‚
â”‚ main_lang           â”‚     â”‚ commit_sha          â”‚
â”‚ ci_provider         â”‚     â”‚ conclusion          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â”‚ (Filtered by config)
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TrainingScenario    â”‚     â”‚TrainingIngestionBuildâ”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ _id                 â”‚â—„â”€â”€â”€â”€â”¤ scenario_id         â”‚
â”‚ name                â”‚     â”‚ raw_build_run_id    â”‚
â”‚ status              â”‚     â”‚ status              â”‚
â”‚ data_source_config  â”‚     â”‚ resource_status     â”‚
â”‚ feature_config      â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ splitting_config    â”‚              â”‚
â”‚ preprocessing_configâ”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â–¼
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚              â”‚TrainingEnrichmentBuildâ”‚
         â”‚              â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
         â”‚              â”‚ scenario_id         â”‚
         â”‚              â”‚ ingestion_build_id  â”‚
         â”‚              â”‚ feature_vector_id   â”‚â”€â”€â”€â–º FeatureVector
         â”‚              â”‚ extraction_status   â”‚
         â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚TrainingDatasetSplit â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ scenario_id         â”‚
â”‚ split_type          â”‚
â”‚ file_path           â”‚
â”‚ record_count        â”‚
â”‚ class_distribution  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Status Enums

```python
class ScenarioStatus(str, Enum):
    QUEUED = "queued"
    FILTERING = "filtering"
    INGESTING = "ingesting"
    INGESTED = "ingested"
    PROCESSING = "processing"
    PROCESSED = "processed"
    SPLITTING = "splitting"
    COMPLETED = "completed"
    FAILED = "failed"

class IngestionStatus(str, Enum):
    PENDING = "pending"
    INGESTING = "ingesting"
    INGESTED = "ingested"
    MISSING_RESOURCE = "missing_resource"
    FAILED = "failed"

class ExtractionStatus(str, Enum):
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    PARTIAL = "partial"
    FAILED = "failed"
```

---

## API Endpoints

**File**: [backend/app/api/training_scenarios.py](backend/app/api/training_scenarios.py)

### Scenario CRUD

| Method | Endpoint | MÃ´ Táº£ |
|--------|----------|-------|
| `POST` | `/training-scenarios` | Create scenario |
| `GET` | `/training-scenarios` | List scenarios |
| `GET` | `/training-scenarios/{id}` | Get scenario detail |
| `PATCH` | `/training-scenarios/{id}` | Update scenario |
| `DELETE` | `/training-scenarios/{id}` | Delete scenario |

### Pipeline Control

| Method | Endpoint | MÃ´ Táº£ |
|--------|----------|-------|
| `POST` | `/training-scenarios/{id}/start-ingestion` | Start Phase 1 |
| `POST` | `/training-scenarios/{id}/start-processing` | Start Phase 2 |
| `POST` | `/training-scenarios/{id}/generate-dataset` | Start Phase 3 |
| `POST` | `/training-scenarios/{id}/retry-ingestion` | Retry failed ingestion |
| `POST` | `/training-scenarios/{id}/retry-processing` | Retry failed processing |

### Builds & Splits

| Method | Endpoint | MÃ´ Táº£ |
|--------|----------|-------|
| `GET` | `/training-scenarios/{id}/builds/import` | List ingestion builds |
| `GET` | `/training-scenarios/{id}/builds/enrichment` | List enrichment builds |
| `GET` | `/training-scenarios/{id}/splits` | List dataset splits |
| `GET` | `/training-scenarios/preview-builds` | Preview builds with filters |

---

## Frontend UI Flow

**Files**: [frontend/src/app/(app)/scenarios/](frontend/src/app/(app)/scenarios/)

### Page Structure

```
/scenarios
â”œâ”€â”€ page.tsx                   # Scenario list
â”œâ”€â”€ upload/                    # BuildSource upload wizard
â”‚   â”œâ”€â”€ page.tsx
â”‚   â””â”€â”€ _components/
â”œâ”€â”€ create/                    # Scenario creation wizard
â”‚   â”œâ”€â”€ page.tsx
â”‚   â””â”€â”€ _components/
â”‚       â”œâ”€â”€ StepDataSource.tsx     # Step 1: Filter config
â”‚       â”œâ”€â”€ StepFeatures.tsx       # Step 2: Feature selection
â”‚       â”œâ”€â”€ StepSplitting.tsx      # Step 3: Split strategy
â”‚       â”œâ”€â”€ StepPreprocessing.tsx  # Step 4: Preprocessing
â”‚       â””â”€â”€ WizardContext.tsx      # Wizard state management
â””â”€â”€ [scenarioId]/
    â”œâ”€â”€ layout.tsx             # Tabs navigation
    â”œâ”€â”€ page.tsx               # Overview
    â”œâ”€â”€ builds/                # Ingestion + Enrichment builds
    â”œâ”€â”€ analysis/              # Feature analysis
    â””â”€â”€ export/                # Download splits
        â””â”€â”€ page.tsx
```

### Create Wizard Flow

```
Step 1: Data Source (Filter builds from warehouse)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Filters                      â”‚  Preview                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ Languages: [Python â–¼]    â”‚â”‚ â”‚ Total: 5,230 builds     â”‚  â”‚
â”‚ â”‚ CI Provider: [All â–¼]     â”‚â”‚ â”‚ Repos: 45               â”‚  â”‚
â”‚ â”‚ Conclusions: [â˜‘ Success] â”‚â”‚ â”‚ Success: 3,800 (72%)    â”‚  â”‚
â”‚ â”‚            [â˜‘ Failure]   â”‚â”‚ â”‚ Failure: 1,430 (28%)    â”‚  â”‚
â”‚ â”‚ Date Range: [2024-01-01] â”‚â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚ â”‚         to: [2024-12-31] â”‚â”‚                              â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚ [Apply Filters]              â”‚
â”‚                                                              â”‚
â”‚                                               [Next: Features]â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 2: Features (Select DAG features + scan metrics)
Step 3: Splitting (Configure train/val/test ratios)
Step 4: Preprocessing (Missing values, normalization)
Step 5: Review & Start
```

### Export Page

```
/scenarios/{id}/export
â”œâ”€â”€ Dataset Summary Card
â”‚   â”œâ”€ Total Splits: 3
â”‚   â”œâ”€ Total Records: 4,500
â”‚   â”œâ”€ Features: 45
â”‚   â””â”€ Total Size: 12.5 MB
â”œâ”€â”€ Split Files Table
â”‚   â”œâ”€ train.parquet (3,150 records, 8.7 MB) [Download]
â”‚   â”œâ”€ val.parquet (675 records, 1.9 MB) [Download]
â”‚   â””â”€ test.parquet (675 records, 1.9 MB) [Download]
â””â”€â”€ Class Distribution Chart
```

---

## Error Handling & Recovery

### Ingestion Errors

| Error Type | Status | Retryable | Action |
|------------|--------|-----------|--------|
| Clone failed (timeout) | FAILED | Yes | `reingest_failed_builds` |
| Worktree creation failed | FAILED | Yes | `reingest_failed_builds` |
| Log download timeout | FAILED | Yes | `reingest_failed_builds` |
| Logs expired (404) | MISSING_RESOURCE | No | Cannot retry |
| Commit not in repo | MISSING_RESOURCE | No | Cannot retry |

### Processing Errors

| Error Type | Status | Retryable | Action |
|------------|--------|-----------|--------|
| Feature extraction failed | FAILED | Yes | `reprocess_failed_builds` |
| Hamilton DAG error | FAILED | Yes | `reprocess_failed_builds` |
| Scan timeout | N/A | No | Scan runs async, skip backfill |

---

## WebSocket Real-time Updates

### Event Types

```python
# Scenario status update
{
    "event": "SCENARIO_UPDATE",
    "scenario_id": "...",
    "status": "processing",
    "message": "Extracting features for 150 builds...",
    "stats": {
        "builds_total": 500,
        "builds_ingested": 450,
        "builds_processed": 150,
    }
}

# Build status update
{
    "event": "SCENARIO_BUILD_UPDATE",
    "scenario_id": "...",
    "build_id": "...",
    "phase": "processing",
    "status": "completed",
}
```

---

## Summary

Training Scenario Pipeline lÃ  há»‡ thá»‘ng 4-phase táº¡o dataset ML tá»« builds:

1. **Build Source Upload**: Admin upload CSV â†’ validate â†’ store to warehouse
2. **Filtering & Ingestion**: Query warehouse vá»›i filters â†’ clone/worktree/logs
3. **Processing**: Extract features (sequential) â†’ dispatch scans (async)
4. **Dataset Generation**: Split â†’ export train/val/test files

**Key Design Decisions**:
- **Warehouse-first**: Builds Ä‘Æ°á»£c validate trÆ°á»›c, lÆ°u vÃ o `raw_build_runs`
- **Filter-then-ingest**: User chá»n builds tá»« warehouse, khÃ´ng tá»« CSV trá»±c tiáº¿p
- **3-phase user control**: User manually triggers Ingestion â†’ Processing â†’ Generation
- **Train/Val/Test splits**: Cáº¥u hÃ¬nh splitting strategy vá»›i ratios tÃ¹y chá»‰nh
- **Sequential processing**: Temporal features yÃªu cáº§u xá»­ lÃ½ tuáº§n tá»±
- **Async scans**: Trivy/SonarQube cháº¡y song song, khÃ´ng block feature extraction
